# 集群模式

redis cluster（redis集群）是 redis 提供的分布式数据库方案，通过数据分片实现了分布式存储，提供了水平扩展能力。

集群模式主要包含以下内容

1. 数据分片

   集群引入了哈希槽位的概念，共16384个哈希槽位，每个槽位只能指派给一个节点，每个键都映射到一个槽位

2. 主从复制

   使用redis中的主从复制实现数据备份

3. 故障转移

   集群模式也实现了故障转移

集群模式的简单配置如下

```
# 在redis.conf中设置
port 6101
cluster-enabled yes #当前节点开启集群模式
cluster-config-file nodes.conf #集群数据节点，记录集群的运行时数据
cluster-node-timeout 5000 #定义了节点之间的通信超时时间阈值，如果一个节点在指定的时间内没有收到另一个节点的响应，则将其标记为失效节点
```

准备6个配置文件，端口号为6101-6106，执行 `redis-cli --cluster create 127.0.0.1:6101 127.0.0.1:6102 127.0.0.1:6103 127.0.0.1:6104 127.0.0.1:6105 127.0.0.1:6106 --cluster-replicas 1` 创建集群

## 槽位管理

### 定义

server 结构体中定义了集群相关属性

+ cluster_enabled：当前节点是否启动集群模式
+ cluster：类型为 `struct clusterState`，存储 cluster 集群信息
+ cluster_configfile：当前节点的数据文件，存储集群运行时数据
+ cluster_slave_no_failover：禁止该节点执行故障转移

集群中的每一个节点都维护一份当前节点视角下的集群状态，该状态存储在 `struct clusterState` 结构体中

```C
//src/cluster.h
typedef struct clusterState {
    clusterNode *myself;  /* This node */
    uint64_t currentEpoch;
    int state;            /* CLUSTER_OK, CLUSTER_FAIL, ... */
    int size;             /* Num of master nodes with at least one slot */
    dict *nodes;          /* Hash table of name -> clusterNode structures */
    dict *shards;         /* Hash table of shard_id -> list (of nodes) structures */
    dict *nodes_black_list; /* Nodes we don't re-add for a few seconds. */
    clusterNode *migrating_slots_to[CLUSTER_SLOTS];
    clusterNode *importing_slots_from[CLUSTER_SLOTS];
    clusterNode *slots[CLUSTER_SLOTS];
    /*...*/
} clusterState;
```

+ myself：自身节点实例
+ currentEpoch：集群当前任期号，用于实现raft算法选举leader
+ state：集群状态
+ nodes：集群节点实例字典，键为ID，值为clusterNode结构体
+ migrating_slots_to：迁出槽位，数组元素为clusterNode。该数组元素不为空，说明该槽位数据正从当前节点迁移到指定节点
+ importing_slots_from：迁入槽位，数组元素为clusterNode。该数组元素不为空，说明该槽位数据正从指定节点迁移到当前节点
+ slots：槽位指派数组，数组元素为clusterNode，即该槽位的数据存储节点

clusterNode结构体负责存放集群中集群节点实例的相关信息

```C
//src/cluster.h
typedef struct clusterNode {
    mstime_t ctime; /* Node object creation time. */
    char name[CLUSTER_NAMELEN]; /* Node name, hex string, sha1-size */
    char shard_id[CLUSTER_NAMELEN]; /* shard id, hex string, sha1-size */
    int flags;      /* CLUSTER_NODE_... */
    uint64_t configEpoch; /* Last configEpoch observed for this node */
    unsigned char slots[CLUSTER_SLOTS/8]; /* slots handled by this node */
    uint16_t *slot_info_pairs; /* Slots info represented as (start/end) pair (consecutive index). */
    int slot_info_pairs_count; /* Used number of slots in slot_info_pairs */
    int numslots;   /* Number of slots handled by this node */
    int numslaves;  /* Number of slave nodes, if this is a master */
    struct clusterNode **slaves; /* pointers to slave nodes */
    struct clusterNode *slaveof; /* pointer to the master node. Note that it
                                    may be NULL even if the node is a slave
                                    if we don't have the master node in our
                                    tables. */
    /*...*/
} clusterNode;
```

+ flags：节点标志，存储节点的状态、属性等
+ name：节点ID，40字节的随机字符串
+ configEpoch：最新写入数据文件的任期号，即最新成功执行故障转移的任期
+ slots：槽位位图，记录该节点负责的槽位
+ slaves：该节点的从节点实例列表
+ slaveof：该节点的主节点实例
+ fail_time：该节点的下线时间
+ ping_sent：当前节点上次给该节点发送 PING 的时间
+ pong_received：当前节点上次收到该节点 PONG 响应的时间
+ data_received：当前节点上次收到该节点任何响应数据的时间
+ voted_time：该节点故障转移时，当前节点上一次投票的时间
+ link：当前节点与该节点的连接
+ fail_reports：下线报告列表，记录所有判定该节点主观下线的主节点，用于客观下线的统计

### 客户端重定向

如果某个集群节点收到客户端的请求，但请求中查询的键不是由当前节点负责的，它将通知客户端进行重定向，向客户端重新发送真正存储该键的节点。当redis节点运行在集群模式下时，`processCommand` 执行命令前会检查请求的键是否存在于当前节点，不存在则向客户端发送重定向命令

```C
//src/server.c +3941
    if (server.cluster_enabled &&//满足以下条件则进行重定向检查：1）开启集群模式；2）收到的命令不是来自于主节点或者AOF客户端；3）是exec命令，或者包含key参数，或者命令具有CMD_MOVABLE_KEYS标识
        !mustObeyClient(c) &&
        !(!(c->cmd->flags&CMD_MOVABLE_KEYS) && c->cmd->key_specs_num == 0 &&
          c->cmd->proc != execCommand))
    {
        int error_code;
        clusterNode *n = getNodeByQuery(c,c->cmd,c->argv,c->argc,//查找键真正的存储节点
                                        &c->slot,&error_code);
        if (n == NULL || n != server.cluster->myself) {//条件成立说明key不再当前节点或者发生其他错误，redis 杂集群模式下不支持跨节点事物
            if (c->cmd->proc == execCommand) {//如果client执行的是EXEC命令，就会被调用，它会放弃事务的执行
                discardTransaction(c);
            } else {//如果client执行的是普通命令，则添加CLIENT_DIRTY_EXEC标记，后续继续执行exec命令时会根据该标记放弃事物执行
                flagTransaction(c);
            }
            clusterRedirectClient(c,n,c->slot,error_code);//根据getNodeByQuery函数返回的error_code的不同值，执行相应的代码分支，主要是把key所属slot对应集群节点的情况返回给客户端
            c->cmd->rejected_calls++;
            return C_OK;
        }
    }
```

`getNodeByQuery` 负责查找键真正的存储节点

```C
//src/cluster.c
/*
	函数执行成功的前提是命令只针对以下情况之一：
	
	单个键（甚至多次，如 RPOPLPUSH mylist mylist）。
	多个键位于相同的哈希槽（hash slot），且哈希槽稳定（没有正在进行的重新分片）。
	函数成功执行后会返回能够处理请求的节点。如果节点不是当前节点（即执行函数的节点），则需要进行重定向。重定向的类型由传递的整数引用 error_code 指定，其值将设置为 CLUSTER_REDIR_ASK 或 CLUSTER_REDIR_MOVED。
	
	当节点是当前节点时，error_code 被设置为 CLUSTER_REDIR_NONE。
	
	如果命令失败，则返回 NULL，并通过 error_code 提供失败的原因，可能的值包括：
	
	CLUSTER_REDIR_CROSS_SLOT：请求包含多个不属于同一哈希槽的键。
	CLUSTER_REDIR_UNSTABLE：请求包含多个属于同一哈希槽的键，但是哈希槽不稳定（正在进行迁移或导入状态，可能是由于重新分片正在进行）。
	CLUSTER_REDIR_DOWN_UNBOUND：请求地址的哈希槽未绑定到任何节点。
	CLUSTER_REDIR_DOWN_STATE 和 CLUSTER_REDIR_DOWN_RO_STATE：集群处于下线状态，但用户尝试执行针对一个或多个键的命令。
*/
clusterNode *getNodeByQuery(client *c, struct redisCommand *cmd, robj **argv, int argc, int *hashslot, int *error_code) {
    /*...*/
    
    /* Set error code optimistically for the base case. */
    if (error_code) *error_code = CLUSTER_REDIR_NONE;

    /* We handle all the cases as if they were EXEC commands, so we have
     * a common code path for everything */
    if (cmd->proc == execCommand) {//收到 exec 命令，从客户端获取mstate
        /* If CLIENT_MULTI flag is not set EXEC is just going to return an
         * error. */
        if (!(c->flags & CLIENT_MULTI)) return myself;
        ms = &c->mstate;
    } else {//为了统一后续处理逻辑，针对单条命令使用mstate进行封装
        /* In order to have a single codepath create a fake Multi State
         * structure if the client is not in MULTI/EXEC state, this way
         * we have a single codepath below. */
        ms = &_ms;
        _ms.commands = &mc;
        _ms.count = 1;
        mc.argv = argv;
        mc.argc = argc;
        mc.cmd = cmd;
    }

    int is_pubsubshard = cmd->proc == ssubscribeCommand ||
            cmd->proc == sunsubscribeCommand ||
            cmd->proc == spublishCommand;

    /* Check that all the keys are in the same hash slot, and obtain this
     * slot and the node associated. */
    for (i = 0; i < ms->count; i++) {//双重循环，针对 mstate 中的每一条命令的每一个 key，检查 key 所在的槽位
        struct redisCommand *mcmd;
        robj **margv;
        int margc, numkeys, j;
        keyReference *keyindex;

        mcmd = ms->commands[i].cmd;
        margc = ms->commands[i].argc;
        margv = ms->commands[i].argv;

        getKeysResult result = GETKEYS_RESULT_INIT;//获取命令中 key 的位置以及数量
        numkeys = getKeysFromCommand(mcmd,margv,margc,&result);
        keyindex = result.keys;
		
        for (j = 0; j < numkeys; j++) {//针对每一个key，检查key所在的槽位
            robj *thiskey = margv[keyindex[j].pos];
            int thisslot = keyHashSlot((char*)thiskey->ptr,//计算方法为用key的CRC16对16383进行取模
                                       sdslen(thiskey->ptr));

            if (firstkey == NULL) {//针对第一个key进行处理
                /* This is the first key we see. Check what is the slot
                 * and node. */
                firstkey = thiskey;
                slot = thisslot;
                n = server.cluster->slots[slot];

                /* Error: If a slot is not served, we are in "cluster down"Pub/Sub
                 * state. However the state is yet to be updated, so this was
                 * not trapped earlier in processCommand(). Report the same
                 * error to the client. */
                if (n == NULL) {
                    getKeysFreeResult(&result);
                    if (error_code)
                        *error_code = CLUSTER_REDIR_DOWN_UNBOUND;
                    return NULL;
                }

                /* If we are migrating or importing this slot, we need to check
                 * if we have all the keys in the request (the only way we
                 * can safely serve the request, otherwise we return a TRYAGAIN
                 * error). To do so we set the importing/migrating state and
                 * increment a counter for every missing key. */
                if (n == myself && //如果查找的节点就是当前节点并且key所属的slot正在迁出，则设置migrating_slot为1
                    server.cluster->migrating_slots_to[slot] != NULL)
                {
                    migrating_slot = 1;
                } else if (server.cluster->importing_slots_from[slot] != NULL) {//如果key所属的slot正在迁入，则设置importing_slot为1
                    importing_slot = 1;
                }
            } else {
                /* If it is not the first key/channel, make sure it is exactly
                 * the same key/channel as the first we saw. */
                if (slot != thisslot) {//在找到第一个key所属的槽位之后，redis要求后续所有的key与第一个key属于同一个槽位，否则报错
                    /* Error: multiple keys from different slots. */
                    getKeysFreeResult(&result);
                    if (error_code)
                        *error_code = CLUSTER_REDIR_CROSS_SLOT;
                    return NULL;                  
                }
                if (importing_slot && !multiple_keys && !equalStringObjects(firstkey,thiskey)) {
                    /* Flag this request as one with multiple different
                     * keys/channels when the slot is in importing state. */
                    multiple_keys = 1;
                }
            }

            /* Migrating / Importing slot? Count keys we don't have.
             * If it is pubsubshard command, it isn't required to check
             * the channel being present or not in the node during the
             * slot migration, the channel will be served from the source
             * node until the migration completes with CLUSTER SETSLOT <slot>
             * NODE <node-id>. */
            int flags = LOOKUP_NOTOUCH | LOOKUP_NOSTATS | LOOKUP_NONOTIFY | LOOKUP_NOEXPIRE;
            if ((migrating_slot || importing_slot) && !is_pubsubshard)
            {//如果key所属slot正在迁出或迁入，并且当前访问的key不在本地数据库，那么增加missing_keys的大小，否则增加existing_keys的大小
                if (lookupKeyReadWithFlags(&server.db[0], thiskey, flags) == NULL) missing_keys++;
                else existing_keys++;
            }
        }
        getKeysFreeResult(&result);
    }

    /* No key at all in command? then we can serve the request
     * without redirections or errors in all the cases. */
    if (n == NULL) return myself;

    uint64_t cmd_flags = getCommandFlags(c);
    /* Cluster is globally down but we got keys? We only serve the request
     * if it is a read command and when allow_reads_when_down is enabled. */
    if (server.cluster->state != CLUSTER_OK) {//在集群状态不正常的情况下，如果配置了cluster_allow_pubsubshard_when_down则允许继续转发 Pub/Sub 操作，如果配置了 allow_reads_when_down 则允许处理读命令，否则报错
        if (is_pubsubshard) {
            if (!server.cluster_allow_pubsubshard_when_down) {
                if (error_code) *error_code = CLUSTER_REDIR_DOWN_STATE;
                return NULL;
            }
        } else if (!server.cluster_allow_reads_when_down) {
            /* The cluster is configured to block commands when the
             * cluster is down. */
            if (error_code) *error_code = CLUSTER_REDIR_DOWN_STATE;
            return NULL;
        } else if (cmd_flags & CMD_WRITE) {
            /* The cluster is configured to allow read only commands */
            if (error_code) *error_code = CLUSTER_REDIR_DOWN_RO_STATE;
            return NULL;
        } else {
            /* Fall through and allow the command to be executed:
             * this happens when server.cluster_allow_reads_when_down is
             * true and the command is not a write command */
        }
    }

    /* Return the hashslot by reference. */
    if (hashslot) *hashslot = slot;

    /* MIGRATE always works in the context of the local node if the slot
     * is open (migrating or importing state). We need to be able to freely
     * move keys among instances in this case. *///如果访问key所属的slot正在做迁入迁出，并且当前命令就是migrate命令，则返回当前节点
    if ((migrating_slot || importing_slot) && cmd->proc == migrateCommand)
        return myself;

    /* If we don't have all the keys and we are migrating the slot, send
     * an ASK redirection or TRYAGAIN. */
    if (migrating_slot && missing_keys) {//key正在迁出并且已经有成功迁出的key的情况下，根据key是否迁移完毕分情况处理
        /* If we have keys but we don't have all keys, we return TRYAGAIN */
        if (existing_keys) {//existing_keys大于0说明当前节点还有正在迁移的key，通知客户端重试
            if (error_code) *error_code = CLUSTER_REDIR_UNSTABLE;
            return NULL;
        } else {//existing_keys等于0说明所有key已经迁移完毕，返回ASK并返回数据迁出的目标节点。客户端的收到ASK后，在访问目标节点前会首先发送ASK命令确定迁移是否完成
            if (error_code) *error_code = CLUSTER_REDIR_ASK;
            return server.cluster->migrating_slots_to[slot];
        }
    }

    /* If we are receiving the slot, and the client correctly flagged the
     * request as "ASKING", we can serve the request. However if the request
     * involves multiple keys and we don't have them all, the only option is
     * to send a TRYAGAIN error. */
    if (importing_slot &&//如果当前节点正在作迁入操作并且客户端发送了ASKING请求
        (c->flags & CLIENT_ASKING || cmd_flags & CMD_ASKING))
    {
        if (multiple_keys && missing_keys) {//需要处理多个key并且存在尚未迁入的key则通知客户端重试
            if (error_code) *error_code = CLUSTER_REDIR_UNSTABLE;
            return NULL;
        } else {
            return myself;//所有key均已迁入，则返回自身
        }
    }

    /* Handle the read-only client case reading from a slave: if this
     * node is a slave and the request is about a hash slot our master
     * is serving, we can reply without redirection. */
    int is_write_command = (cmd_flags & CMD_WRITE) ||
                           (c->cmd->proc == execCommand && (c->mstate.cmd_flags & CMD_WRITE));
    if (((c->flags & CLIENT_READONLY) || is_pubsubshard) && //这里，如果读请求发送到的当前节点是从节点并且该从节点的主节点负责key所在的槽位，则返回当前节点
        !is_write_command &&
        nodeIsSlave(myself) &&
        myself->slaveof == n)
    {
        return myself;
    }

    /* Base case: just return the right node. However if this node is not
     * myself, set error_code to MOVED since we need to issue a redirection. */
    if (n != myself && error_code) *error_code = CLUSTER_REDIR_MOVED;//最后，返回MOVED以及目标节点n
    return n;
}
```

### 槽位迁移

槽位迁移的命令如下 `redis-cli --cluster reshard <host>:<port> --cluster-from srcid --cluster-to dstid --cluster-slots 100`

+ \<host>:\<port> 指定集群中任意节点的ip和端口号即可，`redis-cli` 会自动获取集群信息
+ --cluster-from、--cluster-to 分别指定迁出节点id和迁入节点id
+ --cluster-slots 指定要迁移的槽位数量

槽位迁移包含以下过程：

1. 发送命令给迁入节点，指定槽位迁移的迁出节点

   CLUSTER SETSLOT \<SLOT> IMPORTING srcid

   迁入节点收到命令后会设置 `server.cluster.importing_slots_from` 指向迁出节点，代表该槽位的数据正从迁出节点迁入当前节点

2. 发送命令给迁出节点，指定槽位迁移的迁入节点

   CLUSTER SETSLOT \<SLOT> MIGRATING dstid

   迁入节点收到命令后会设置 `server.cluster.migrating_slots_to` 指向迁入节点，代表该槽位的数据正从当前节点迁出到迁入节点

3. 发送命令给迁出节点，获取待迁出的keys

   CLUSTER GETKEYSINSLOT \<SLOT> \<count>

4. 发送命令给迁出节点，开始迁移数据

   MIGRATE host port "" destination-db timeout [COPY] [REPLACE] [AUTH password] [AUTH2 username password] KEYS key1 key2 key3 ...

   COPY：复制数据，不删除迁出节点中的键

   REPLACE：替换数据，替换迁入节点中的键

   KEYS：如果 key 参数是一个空字符串，则迁移 KEYS 后的所有键

   AUTH：使用密码访问迁入节点

   AUTH2：redis6 以上提供，使用用户名密码访问迁入节点

5. 当槽位上所有键都迁入到迁入节点之后，给集群中所有主节点发送 `CLUSTER SLOT <slot> NODE <node-id>`

   集群中的主节点收到该命令后执行以下操作：

   + 如果当前节点是迁出节点，并且槽位数据已经全部迁出，则置空 `server.cluster.migrating_slots_to[slot]`，代表该槽位数据迁出完成
   + 如果当前节点是迁入节点，并且槽位数据已经全部迁入，则置空 `server.cluster.importing_slots_from[slot]`，代表该槽位数据迁入完成
   + 更新槽位指派数组 `clusterState.slots` 以及节点实例的位图信息 `clusterNode.slots`

下面是 migrate 命令的实现

```C
//src/cluster.c
/*
命令个是如下
 * MIGRATE host port key dbid timeout [COPY | REPLACE | AUTH password |
 *         AUTH2 username password]
 *
 * On in the multiple keys form:
 *
 * MIGRATE host port "" dbid timeout [COPY | REPLACE | AUTH password |
 *         AUTH2 username password] KEYS key1 key2 ... keyN
 主要流程：
 1.解析并检查参数
 2.创建ov和kv数组，调用 lookupKeyRead 获取实际存在的键值
 3.获区到目的节点的连接
 4.建立缓冲区，用迁移命令、key、ttl以及value填充缓冲区
 5.同步发送数据
 6.同步获取目的节点的应答，如果出错则进行错误处理
 */
void migrateCommand(client *c) {
   

    /* To support the KEYS option we need the following additional state. */
    int first_key = 3; /* Argument index of the first key. */
    int num_keys = 1;  /* By default only migrate the 'key' argument. */

    /* Parse additional options */
    /*解析命令中的参数，根据解析情况设置响应变量*/
    if (timeout <= 0) timeout = 1000;

    /* Check if the keys are here. If at least one key is to migrate, do it
     * otherwise if all the keys are missing reply with "NOKEY" to signal
     * the caller there was nothing to migrate. We don't return an error in
     * this case, since often this is due to a normal condition like the key
     * expiring in the meantime. */
    ov = zrealloc(ov,sizeof(robj*)*num_keys);//保存要迁移的value
    kv = zrealloc(kv,sizeof(robj*)*num_keys);//保存要迁移的key
    int oi = 0;

    for (j = 0; j < num_keys; j++) {
        if ((ov[oi] = lookupKeyRead(c->db,c->argv[first_key+j])) != NULL) {
            kv[oi] = c->argv[first_key+j];
            oi++;
        }
    }
    num_keys = oi;
    if (num_keys == 0) {
        zfree(ov); zfree(kv);
        addReplySds(c,sdsnew("+NOKEY\r\n"));
        return;
    }

try_again:
    write_error = 0;

    /* Connect */
    cs = migrateGetSocket(c,c->argv[1],c->argv[2],timeout);//获取到目的节点的连接
    if (cs == NULL) {
        zfree(ov); zfree(kv);
        return; /* error sent to the client by migrateGetSocket() */
    }

    rioInitWithBuffer(&cmd,sdsempty());//初始化buffer，用于填充发送给目的节点的命令、key和value

    /* Authentication */
    if (password) {//填充AUTH命令用于认证
        /*...*/
    }

    /* Send the SELECT command if the current DB is not already selected. */
    int select = cs->last_dbid != dbid; /* Should we emit SELECT? */
    if (select) {//填充select命令
        serverAssertWithInfo(c,NULL,rioWriteBulkCount(&cmd,'*',2));
        serverAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,"SELECT",6));
        serverAssertWithInfo(c,NULL,rioWriteBulkLongLong(&cmd,dbid));
    }

    int non_expired = 0; /* Number of keys that we'll find non expired.
                            Note that serializing large keys may take some time
                            so certain keys that were found non expired by the
                            lookupKey() function, may be expired later. */

    /* Create RESTORE payload and generate the protocol to call the command. */
    for (j = 0; j < num_keys; j++) {
        long long ttl = 0;
        long long expireat = getExpire(c->db,kv[j]);

        if (expireat != -1) {//检查key是否过期，对于过期的key则没有必要迁移
            ttl = expireat-commandTimeSnapshot();
            if (ttl < 0) {
                continue;
            }
            if (ttl < 1) ttl = 1;
        }

        /* Relocate valid (non expired) keys and values into the array in successive
         * positions to remove holes created by the keys that were present
         * in the first lookup but are now expired after the second lookup. */
        ov[non_expired] = ov[j];
        kv[non_expired++] = kv[j];

        serverAssertWithInfo(c,NULL,
            rioWriteBulkCount(&cmd,'*',replace ? 5 : 4));

        if (server.cluster_enabled)//如果启用集群模式，则填充 RESTORE-ASKING 命令发送给目标节点
            serverAssertWithInfo(c,NULL,
                rioWriteBulkString(&cmd,"RESTORE-ASKING",14));
        else
            serverAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,"RESTORE",7));
        serverAssertWithInfo(c,NULL,sdsEncodedObject(kv[j]));
        serverAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,kv[j]->ptr, //填充key
                sdslen(kv[j]->ptr)));
        serverAssertWithInfo(c,NULL,rioWriteBulkLongLong(&cmd,ttl));//填充ttl

        /* Emit the payload argument, that is the serialized object using
         * the DUMP format. */
        createDumpPayload(&payload,ov[j],kv[j],dbid);//将迁移key的value序列化，以便于传输。在序列化的结果中，。等目的节点收到迁移数据后，也会检查增加RDB版本号和CRC校验和createDumpPayload函数会这两部分内容
        serverAssertWithInfo(c,NULL,
            rioWriteBulkString(&cmd,payload.io.buffer.ptr,//填充序列化后的 value
                               sdslen(payload.io.buffer.ptr)));
        sdsfree(payload.io.buffer.ptr);

        /* Add the REPLACE option to the RESTORE command if it was specified
         * as a MIGRATE option. */
        if (replace)
            serverAssertWithInfo(c,NULL,rioWriteBulkString(&cmd,"REPLACE",7));
    }

    /* Fix the actual number of keys we are migrating. */
    num_keys = non_expired;

    /* Transfer the query to the other node in 64K chunks. */
    errno = 0;//以64KB的粒度将缓冲区中的内容发送给目的节点
    {
        sds buf = cmd.io.buffer.ptr;
        size_t pos = 0, towrite;
        int nwritten = 0;

        while ((towrite = sdslen(buf)-pos) > 0) {
            towrite = (towrite > (64*1024) ? (64*1024) : towrite);
            nwritten = connSyncWrite(cs->conn,buf+pos,towrite,timeout);//发送数据
            if (nwritten != (signed)towrite) {
                write_error = 1;
                goto socket_err;
            }
            pos += nwritten;
        }
    }

    char buf0[1024]; /* Auth reply. */
    char buf1[1024]; /* Select reply. */
    char buf2[1024]; /* Restore reply. */

    /* Read the AUTH reply if needed. */
    if (password && connSyncReadLine(cs->conn, buf0, sizeof(buf0), timeout) <= 0)
        goto socket_err;

    /* Read the SELECT reply if needed. */
    if (select && connSyncReadLine(cs->conn, buf1, sizeof(buf1), timeout) <= 0)
        goto socket_err;

    /* Read the RESTORE replies. */
    int error_from_target = 0;
    int socket_error = 0;
    int del_idx = 1; /* Index of the key argument for the replicated DEL op. */

    /* Allocate the new argument vector that will replace the current command,
     * to propagate the MIGRATE as a DEL command (if no COPY option was given).
     * We allocate num_keys+1 because the additional argument is for "DEL"
     * command name itself. */
    if (!copy) newargv = zmalloc(sizeof(robj*)*(num_keys+1));

    for (j = 0; j < num_keys; j++) {
        if (connSyncReadLine(cs->conn, buf2, sizeof(buf2), timeout) <= 0) {//针对迁移的每个键值对，读取目标节点的返回结果
            socket_error = 1;
            break;
        }
        if ((password && buf0[0] == '-') ||
            (select && buf1[0] == '-') ||
            buf2[0] == '-')
        {
            /* On error assume that last_dbid is no longer valid. */
            if (!error_from_target) {
                cs->last_dbid = -1;
                char *errbuf;
                if (password && buf0[0] == '-') errbuf = buf0;
                else if (select && buf1[0] == '-') errbuf = buf1;
                else errbuf = buf2;

                error_from_target = 1;
                addReplyErrorFormat(c,"Target instance replied with error: %s",//将发生的错误返回给调用方
                    errbuf+1);
            }
        } else {
            if (!copy) {
                /* No COPY option: remove the local key, signal the change. */
                dbDelete(c->db,kv[j]);
                signalModifiedKey(c,c->db,kv[j]);
                notifyKeyspaceEvent(NOTIFY_GENERIC,"del",kv[j],c->db->id);
                server.dirty++;

                /* Populate the argument vector to replace the old one. */
                newargv[del_idx++] = kv[j];
                incrRefCount(kv[j]);
            }
        }
    }

   /*错误处理*/
}
```

目的节点收到源节点发送的 `RESTORE-ASKING` 命令后，会调用 `restoreCommand` 处理接收到的数据

```C
//src/cluster.c
void restoreCommand(client *c) {
    long long ttl, lfu_freq = -1, lru_idle = -1, lru_clock = -1;
    rio payload;
    int j, type, replace = 0, absttl = 0;
    robj *obj;

    /* Parse additional options *///命令参数解析，包括 replace 标记、ttl等
    for (j = 4; j < c->argc; j++) {
        /*...*/

    /* Make sure this key does not already exist here... */
    robj *key = c->argv[1];
    if (!replace && lookupKeyWrite(c->db,key) != NULL) {//如果未设置 replace 标记并且key已存在则报错
        addReplyErrorObject(c,shared.busykeyerr);
        return;
    }

    /* Check if the TTL value makes sense *///检查ttl是否合法
    if (getLongLongFromObjectOrReply(c,c->argv[2],&ttl,NULL) != C_OK) {
        return;
    } else if (ttl < 0) {
        addReplyError(c,"Invalid TTL value, must be >= 0");
        return;
    }

    /* Verify RDB version and data checksum. *///数据校验
    if (verifyDumpPayload(c->argv[3]->ptr,sdslen(c->argv[3]->ptr),NULL) == C_ERR)
    {
        addReplyError(c,"DUMP payload version or checksum are wrong");
        return;
    }

    rioInitWithBuffer(&payload,c->argv[3]->ptr);//解析实际的数据类型以及数据的值
    if (((type = rdbLoadObjectType(&payload)) == -1) ||
        ((obj = rdbLoadObject(type,&payload,key->ptr,c->db->id,NULL)) == NULL))
    {
        addReplyError(c,"Bad data format");
        return;
    }

    /* Remove the old key if needed. */
    int deleted = 0;
    if (replace)//如果设置了replace标记则删除已存在的key
        deleted = dbDelete(c->db,key);

    if (ttl && !absttl) ttl+=commandTimeSnapshot();
    if (ttl && checkAlreadyExpired(ttl)) {//ttl超时检查，如果已经超时则直接返回应答，不再执行后续的添加逻辑
        if (deleted) {
            robj *aux = server.lazyfree_lazy_server_del ? shared.unlink : shared.del;
            rewriteClientCommandVector(c, 2, aux, key);
            signalModifiedKey(c,c->db,key);
            notifyKeyspaceEvent(NOTIFY_GENERIC,"del",key,c->db->id);
            server.dirty++;
        }
        decrRefCount(obj);
        addReply(c, shared.ok);
        return;
    }

    /* Create the key and set the TTL if any */
    dbAdd(c->db,key,obj);//将key和value写入数据库，并设置ttl
    if (ttl) {
        setExpire(c,c->db,key,ttl);
        if (!absttl) {
            /* Propagate TTL as absolute timestamp */
            robj *ttl_obj = createStringObjectFromLongLong(ttl);
            rewriteClientCommandArgument(c,2,ttl_obj);
            decrRefCount(ttl_obj);
            rewriteClientCommandArgument(c,c->argc,shared.absttl);
        }
    }
    objectSetLRUOrLFU(obj,lfu_freq,lru_idle,lru_clock,1000);//写入lru、lfu信息
    signalModifiedKey(c,c->db,key);
    notifyKeyspaceEvent(NOTIFY_GENERIC,"restore",key,c->db->id);
    addReply(c,shared.ok);
    server.dirty++;
}
```

## 启动过程

### 节点启动

在 initServer 函数中，如果以集群模式启动，则会调用 `clusterInit` 函数执行相关初始化动作

```C
//src/server.c +7303
    if (server.cluster_enabled) {
        clusterInit();
    }
```

其中主要执行了以下逻辑：

1. 初始化 `server.cluster` 变量
2. 尝试锁住数据文件，确保只有当前进程可以修改
3. 尝试加载集群配置。如果配置文件不存在或者大小为0，在创建自身的实例节点 `server.cluster->myself` 并添加到 `server.cluster->nodes` 中
4. 初始化 `server.db->slots_to_keys` ，用于记录每一个 slot 中的key
5. 更新当前节点的 tcp_port、tls_port、cport
6. 除任何正在进行中的手动故障转移过程，将系统恢复到正常的运行状态

之后调用 `clusterInitListeners` 在 `server.cluster_port` 上开启监听，将监听套接字的读事件回调函数设置为 `clusterAcceptHandler`

通过 `edis-cli --cluster create` 可以创建集群，主要包括3个步骤完成了集群的创建：1）节点握手；2）分配槽位；3）建立主从关系

### 节点握手

`CLUSTER MEET target-ip target-port` 命令可以让两个节点互相认识。作为 CLUSTER 命令的子命令，MEET 命令在 `clusterCommand` 中处理。主要逻辑为调用 `clusterStartHandshake` 创建一个节点实例并前加到 `erver.cluster->nodes` 字典中。由于此时还不知道目标节点的name，所以这里创建一个临时的随机字符串作为name。在与目标节点建立网络连接并成功通信后，会从目标节点的响应中获取name并更新。

### 指派槽位

将16348个槽位分派给不同的主节点（其余节点作为从节点），命令如下

`CLUSTER ADDSLOTS <slot> [slot] ...`

集群节点收到命令后会调用 `clusterAddSlot` 更新自身的槽位位图以及槽位指派数组 `server.cluster->slots`

### 建立主从关系

使用如下命令指定集群节点建立主从关系：

`CLUSTER REPLICATE node-id`

节点收到命令后会调用 `clusterSetMaster` 成为 `node-id` 的从节点，该函数执行如下逻辑：

1. 如果当前节点是主节点，则切换为从节点（更新 flags 标志），清除所有负责的槽位
2. 如果是从节点，则清除实例的 slaveof 属性
3. 调用 `replicationSetMaster` 与指定节点建立主从关系
4. 调用 `resetManualFailover` 重置手动故障转移状态

此时，集群已经启动成功，但是整个集群的信息尚未完全同步。

## 集群节点通信

集群刚启动时，各个节点并没有整个集群的完整信息。redis 使用 Gossip 算法同步集群信息。

### Gossip算法

**Gossip算法**是一种分布式通信协议，用于在节点网络中传播信息。它通常用于对等系统、分布式数据库和其他分散式架构中。该算法通过随机选择节点并传播消息来实现信息的分发和同步。每个节点周期性地选择一个随机节点，并将自己知道的信息发送给该节点。这个过程在整个网络中持续进行，直到所有节点都知道了所有信息或者达到了指定的终止条件。Gossip算法具有高度的分散性和容错性，适用于大规模系统中的信息传播和同步任务。

### 消息定义

`clusterMsg` 定义了集群节点之间的消息格式

```C
//src/cluster.h
typedef struct {
    char sig[4];        /* Signature "RCmb" (Redis Cluster message bus). */
    uint32_t totlen;    /* Total length of this message */
    uint16_t ver;       /* Protocol version, currently set to 1. */
    uint16_t port;      /* Primary port number (TCP or TLS). */
    /*...*/
    union clusterMsgData data;
} clusterMsg;
```

消息分为消息头和消息体，以下为消息头

+ sig：固定为 RCmb
+ totlen：消息总长度
+ ver：消息协议版本，目前是1
+ tyjpe：消息类型，主要关注以下类型
  + CLUSTERMSG_TYPE_MEET：要求进行握手操作的消息
  + CLUSTERMSG_TYPE_PING：定时心跳消息
  + CLUSTERMSG_TYPE_PONG：心跳响应消息或者广播消息
  + CLUSTERMSG_TYPE_FAIL：节点客观下线的广播消息
  + CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST：故障转移选举时的投票请求消息
  + CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK：故障转移选举时的投票响应消息
+ flags、currentEpoch、configEpoch：发送节点标志、发送节点最新任期、发送节点最新写入文件的任期
+ sender、myslots、slaveof、myip、cport：发送节点 name、槽位位图、主节点 name、发送节点 ip、发送节点的tcp集群总线端口

data 即消息体，类型为 clusterMsgData，这是一个联合类型，不同的消息使用不同的属性

```C
//src/cluster.h
union clusterMsgData {
    /* PING, MEET and PONG */
    struct {
        /* Array of N clusterMsgDataGossip structures */
        clusterMsgDataGossip gossip[1];
        /* Extension data that can optionally be sent for ping/meet/pong
         * messages. We can't explicitly define them here though, since
         * the gossip array isn't the real length of the gossip data. */
    } ping;

    /* FAIL */
    struct {
        clusterMsgDataFail about;
    } fail;

    /*...*/
};
```

+ ping：clusterMsgDataGossip 类型，存放随机实例和主观下线节点的实例信息，用于CLUSTERMSG_TYPE_PONG、CLUSTERMSG_TYPE_MEET、CLUSTERMSG_TYPE_PING 消息
+ fail：clusterMsgDataFail 类型，节点客观下线通知，存放客观下线节点的 name，用于 CLUSTERMSG_TYPE_FAIL 消息

### 建立连接

`clusterCron` 是集群模式下的定时函数，每100毫秒执行一次，其中包含了连接建立的逻辑。

```C
//src/cluster.c +4620
    di = dictGetSafeIterator(server.cluster->nodes);
    while((de = dictNext(di)) != NULL) {
        clusterNode *node = dictGetVal(de);
        /* We free the inbound or outboud link to the node if the link has an
         * oversized message send queue and immediately try reconnecting. */
        clusterNodeCronFreeLinkOnBufferLimitReached(node);
        /* The protocol is that function(s) below return non-zero if the node was
         * terminated.
         */
        if(clusterNodeCronHandleReconnect(node, handshake_timeout, now)) continue;//实现连接第一次建立以及重新连接的逻辑
    }
    dictReleaseIterator(di);

static int clusterNodeCronHandleReconnect(clusterNode *node, mstime_t handshake_timeout, mstime_t now) {
    /* Not interested in reconnecting the link with myself or nodes
     * for which we have no address. */
    if (node->flags & (CLUSTER_NODE_MYSELF|CLUSTER_NODE_NOADDR)) return 1;

    if (node->flags & CLUSTER_NODE_PFAIL)
        server.cluster->stats_pfail_nodes++;

    /* A Node in HANDSHAKE state has a limited lifespan equal to the
     * configured node timeout. */
    if (nodeInHandshake(node) && now - node->ctime > handshake_timeout) {
        clusterDelNode(node);
        return 1;
    }

    if (node->link == NULL) {
        clusterLink *link = createClusterLink(node);
        link->conn = connCreate(connTypeOfCluster());
        connSetPrivateData(link->conn, link);
        if (connConnect(link->conn, node->ip, node->cport, server.bind_source_addr,
                    clusterLinkConnectHandler) == C_ERR) {//发起连接，设置连接建立后的回调函数为 clusterLinkConnectHandler
            /* We got a synchronous error from connect before
             * clusterSendPing() had a chance to be called.
             * If node->ping_sent is zero, failure detection can't work,
             * so we claim we actually sent a ping now (that will
             * be really sent as soon as the link is obtained). */
            if (node->ping_sent == 0) node->ping_sent = mstime();
            serverLog(LL_DEBUG, "Unable to connect to "
                "Cluster Node [%s]:%d -> %s", node->ip,
                node->cport, server.neterr);

            freeClusterLink(link);
            return 0;
        }
    }
    return 0;
}
```

在 `clusterLinkConnectHandler` 将套接字的读事件处理函数设置为 `clusterReadHandler` 用于处理其他节点返回的响应。同时，在节点启动时会注册集群监听套接字用于处理其他节点的连接请求，此时在新请求建立后，redis 设置的套接字的读事件处理函数也是 `clusterReadHandler`，此时该函数负责处理其他节点的请求。

### 握手过程

当某一个节点与其他节点建立连接之后会调用 `clusterLinkConnectHandler`，该函数会发送 `CLUSTERMSG_TYPE_MEET` 给目标节点，目标节点会返回 `CLUSTERMSG_TYPE_PONG` 作为应答。收到请求后，目标节点会调用 `clusterReadHandler` 处理请求。而其中最终调用的是 `clusterProcessPacket` 处理具体数据

```C
//src/cluster.c
int clusterProcessPacket(clusterLink *link) {
	/*...*/
    if (type == CLUSTERMSG_TYPE_PING || type == CLUSTERMSG_TYPE_MEET) {
        /*...*/
        /* Anyway reply with a PONG */
        clusterSendPing(link,CLUSTERMSG_TYPE_PONG);//收到PING或者MEET消息总是回复PONG
    }
    
    /* PING, PONG, MEET: process config information. */
    if (type == CLUSTERMSG_TYPE_PING || type == CLUSTERMSG_TYPE_PONG ||
        type == CLUSTERMSG_TYPE_MEET)
    {
        serverLog(LL_DEBUG,"%s packet received: %.40s",
            clusterGetMessageTypeString(type),
            link->node ? link->node->name : "NULL");
        if (!link->inbound) {
            if (nodeInHandshake(link->node)) {//如果目标节点处于握手状态
               /*...*/

                /* First thing to do is replacing the random name with the
                 * right node name if this was a handshake stage. */
                clusterRenameNode(link->node, hdr->sender);//使用消息数据中发送节点的name更新本节点中目标实例的name（之前处理 CLUSTER MEET 命令时由于不知道目标节点的name所以赋了一个随机值，这里更新为正确的值）
                serverLog(LL_DEBUG,"Handshake with node %.40s completed.",
                    link->node->name);
                link->node->flags &= ~CLUSTER_NODE_HANDSHAKE;//握手完成，清除握手标志
                link->node->flags |= flags&(CLUSTER_NODE_MASTER|CLUSTER_NODE_SLAVE);
                clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG);
        /* Get info from the gossip section */
        if (sender) {
            clusterProcessGossipSection(hdr,link);//处理 clusterMsgDataGossip 数据
            clusterProcessPingExtensions(hdr,link);
        }
    } /*...*/
```

### 定时消息

当 redis 工作在集群模式下时会定期调用 `clusterCron` 函数，该函数中会定时向其他节点发送 CLUSTERMSG_TYPE_PING 消息，这是 redis 中 Gossip 协议的实现。

```C
//src/cluster.c +4633
    /* Ping some random node 1 time every 10 iterations, so that we usually ping
     * one random node every second. */
    if (!(iteration % 10)) {//iteration 是静态局部变量，clusterCron 每秒执行10次，所以集群消息每秒发送一次
        int j;

        /* Check a few random nodes and ping the one with the oldest
         * pong_received time. */
        for (j = 0; j < 5; j++) {//随机选取5个目标节点，并从中选择上次收到PONG响应的最早的节点
            de = dictGetRandomKey(server.cluster->nodes);
            clusterNode *this = dictGetVal(de);

            /* Don't ping nodes disconnected or with a ping currently active. */
            if (this->link == NULL || this->ping_sent != 0) continue;
            if (this->flags & (CLUSTER_NODE_MYSELF|CLUSTER_NODE_HANDSHAKE))
                continue;
            if (min_pong_node == NULL || min_pong > this->pong_received) {
                min_pong_node = this;
                min_pong = this->pong_received;
            }
        }
        if (min_pong_node) {//发送PING给选中的节点
            serverLog(LL_DEBUG,"Pinging node %.40s", min_pong_node->name);
            clusterSendPing(min_pong_node->link, CLUSTERMSG_TYPE_PING);
        }
    }
```

`clusterSendPing` 负责发送集群信息

````C
//src/cluster.c
/*
	link：目标节点连接
	type：消息类型
*/
void clusterSendPing(clusterLink *link, int type) {
    /*...*/
    int freshnodes = dictSize(server.cluster->nodes)-2;
    
    wanted = floor(dictSize(server.cluster->nodes)/10);//clusterMsgDataGossip 中的随机实例的数量，为实例总数量的1/10，不小于3但是必须小于等于实例数量减2
    if (wanted < 3) wanted = 3;
    if (wanted > freshnodes) wanted = freshnodes;

    /*...*/
    clusterMsgSendBlock *msgblock = createClusterMsgSendBlock(type, estlen);//申请内存，初始化消息结构体，并将自身消息添加到消息头中
    clusterMsg *hdr = &msgblock->msg;

    if (!link->inbound && type == CLUSTERMSG_TYPE_PING)
        link->node->ping_sent = mstime();

    /* Populate the gossip fields */
    int maxiterations = wanted*3;//从当前节点的实例字典中获取 wanted 个节点并添加到消息中，这里会过滤自身、主观下线或者握手阶段或者未知的节点
    while(freshnodes > 0 && gossipcount < wanted && maxiterations--) {
        dictEntry *de = dictGetRandomKey(server.cluster->nodes);
        clusterNode *this = dictGetVal(de);

        //*...*/

        /* Add it */
        clusterSetGossipEntry(hdr,gossipcount,this);//将选中的节点添加到消息中
        this->last_in_ping_gossip = cluster_pings_sent;
        freshnodes--;
        gossipcount++;
    }

    /* If there are PFAIL nodes, add them at the end. */
    if (pfail_wanted) {//将当前节点实例字典中所有主观下线的节点添加到消息中，以便尽快地是主观下线的节点转换为客观下线
        dictIterator *di;
        dictEntry *de;

        di = dictGetSafeIterator(server.cluster->nodes);
        while((de = dictNext(di)) != NULL && pfail_wanted > 0) {
            clusterNode *node = dictGetVal(de);
            if (node->flags & CLUSTER_NODE_HANDSHAKE) continue;
            if (node->flags & CLUSTER_NODE_NOADDR) continue;
            if (!(node->flags & CLUSTER_NODE_PFAIL)) continue;
            clusterSetGossipEntry(hdr,gossipcount,node);
            gossipcount++;
            /* We take the count of the slots we allocated, since the
             * PFAIL stats may not match perfectly with the current number
             * of PFAIL nodes. */
            pfail_wanted--;
        }
        dictReleaseIterator(di);
    }

    /* Compute the actual total length and send! */
    uint32_t totlen = 0;//之前的消息长度是按照最大可能的大小估算的，这里重新计算本次消息的真正大小
    totlen += writePingExt(hdr, gossipcount);
    totlen += sizeof(clusterMsg)-sizeof(union clusterMsgData);
    totlen += (sizeof(clusterMsgDataGossip)*gossipcount);
    serverAssert(gossipcount < USHRT_MAX);
    hdr->count = htons(gossipcount);
    hdr->totlen = htonl(totlen);

    clusterSendMessage(link,msgblock);//发送消息
    clusterMsgSendBlockDecrRefCount(msgblock);
}
````

`CLUSTERMSG_TYPE_MEET`、`CLUSTERMSG_TYPE_PING`、`CLUSTERMSG_TYPE_PONG` 都包含随机节点实例信息和下线节点实例信息。

在收到 `clusterMsgDataGossip` 消息后，如果 redis 发现自己不认识的节点，会将该节点添加到自己的实例字典中，最终会与这些“新认识的”节点建立连接。

集群启动时，各个节点只与种子节点执行了 MEET 操作，种子节点会将自己认识的节点发送给其他节点，最终所有节点都会互相认识。

## 故障转移

与哨兵模式类似，当集群模式下检测到某个节点下线后会发起选举选择一个 leader 节点，由 leader 节点完成故障转移。

### 节点下线

集群判定节点下线需要经过主观下线和客观下线两个过程。`clusterCron` 负责检查节点的下线状态

```C
//src/cluster.c +4741
        /* Check if this node looks unreachable.
         * Note that if we already received the PONG, then node->ping_sent
         * is zero, so can't reach this code at all, so we don't risk of
         * checking for a PONG delay if we didn't sent the PING.
         *
         * We also consider every incoming data as proof of liveness, since
         * our cluster bus link is also used for data: under heavy data
         * load pong delays are possible. *///这里是一个循环，针对节点实例进行以下判断
        mstime_t node_delay = (ping_delay < data_delay) ? ping_delay ://取 ping_delay、data_delay 中的较小值作为 node_delay
                                                          data_delay;

        if (node_delay > server.cluster_node_timeout) {//如果 node_delay 大于 server.cluster_node_timeout，则认为对应的节点主观下线
            /* Timeout reached. Set the node as possibly failing if it is
             * not already in this state. */
            if (!(node->flags & (CLUSTER_NODE_PFAIL|CLUSTER_NODE_FAIL))) {
                serverLog(LL_DEBUG,"*** NODE %.40s possibly failing",
                    node->name);
                node->flags |= CLUSTER_NODE_PFAIL;
                update_state = 1;
            }
        }
    }
```

集群节点会定时发送 `CLUSTERMSG_TYPE_PING` 给集群中的其他节点，其中包含了本节点实例字典中所有主观下线节点的信息。集群节点会统计集群中报告某个节点主观下线的主节点数量，从而判定该节点是否客观下线。

`clusterProcessGossipSection` 收到其他节点发送的主观下线的节点，将发送节点添加到其判定的主观下线节点的实例的下线报告列表中（fail_reports 属性）

```C
//src/cluster.c
void clusterProcessGossipSection(clusterMsg *hdr, clusterLink *link) {
    uint16_t count = ntohs(hdr->count);
    clusterMsgDataGossip *g = (clusterMsgDataGossip*) hdr->data.ping.gossip;//消息发送节点将自身具有的实例信息填充到 clusterMsgDataGossip 中，这里 gossip 是一个数组，每一个元素表示一个发送节点中的实例状态
    clusterNode *sender = link->node ? link->node : clusterLookupNode(hdr->sender, CLUSTER_NAMELEN);//消息发送节点

    while(count--) {
        uint16_t flags = ntohs(g->flags);
        /*...*/
        /* Update our state accordingly to the gossip sections */
        node = clusterLookupNode(g->nodename, CLUSTER_NAMELEN);//获取被报告的节点在当前节点中的实例
        if (node) {
            /* We already know this node.
               Handle failure reports, only when the sender is a master. */
            if (sender && nodeIsMaster(sender) && node != myself) {//如果消息的发送节点是主节点并且被报告状态的节点不是当前节点
                if (flags & (CLUSTER_NODE_FAIL|CLUSTER_NODE_PFAIL)) {//发送节点判定 g 代表的节点已经主观下线
                    if (clusterNodeAddFailureReport(node,sender)) {//将发送节点添加到 g 的下线报告列表fail_reports中
                        serverLog(LL_VERBOSE,
                            "Node %.40s (%s) reported node %.40s (%s) as not reachable.",
                            sender->name, sender->human_nodename, node->name, node->human_nodename);
                    }
                    markNodeAsFailingIfNeeded(node);//统计判定 node 主观下线的节点数量，判读是否可以将该节点设置为客观下线
                } else {
                    if (clusterNodeDelFailureReport(node,sender)) {
                        serverLog(LL_VERBOSE,
                            "Node %.40s (%s) reported node %.40s (%s) is back online.",
                            sender->name, sender->human_nodename, node->name, node->human_nodename);
                    }
                }
            }

           /*...*/

        /* Next node */
        g++;
    }
}
    
void markNodeAsFailingIfNeeded(clusterNode *node) {
    int failures;
    int needed_quorum = (server.cluster->size / 2) + 1;//取集群中至少负责一个槽位的主节点数量的二分之一加1作为客观下线判定阈值

    if (!nodeTimedOut(node)) return; /* We can reach it. */
    if (nodeFailed(node)) return; /* Already FAILing. */

    failures = clusterNodeFailureReportsCount(node);//获取该节点下线报告列表中的元素数量，该数量即为判定该节点主观下线的主节点的数量
    /* Also count myself as a voter if I'm a master. */
    if (nodeIsMaster(myself)) failures++;//当前节点是主节点还要再加一
    if (failures < needed_quorum) return; /* No weak agreement from masters. *///未达到客观下线阈值则退出

    serverLog(LL_NOTICE,
        "Marking node %.40s (%s) as failing (quorum reached).", node->name, node->human_nodename);

    /* Mark the node as failing. */
    node->flags &= ~CLUSTER_NODE_PFAIL;
    node->flags |= CLUSTER_NODE_FAIL;//添加客观下线标记
    node->fail_time = mstime();

    /* Broadcast the failing node name to everybody, forcing all the other
     * reachable nodes to flag the node as FAIL.
     * We do that even if this node is a replica and not a master: anyway
     * the failing state is triggered collecting failure reports from masters,
     * so here the replica is only helping propagating this status. */
    clusterSendFail(node->name);//将客观下线判定结果通知其他节点
    clusterDoBeforeSleep(CLUSTER_TODO_UPDATE_STATE|CLUSTER_TODO_SAVE_CONFIG);
}
```

### 选举过程

当某个主节点客观下线后，就需要选举 leader 进行故障转移。选举过程设计如下属性

+ failover_auth_sent：本次选举是否已发送投票请求
+ failover_auth_time：本次选举或者（本次选举已失败）下次选举开始的时间
+ failover_auth_rank：节点优先级，数值越大优先级越低，故障转移时重新发起选举前等待的时间越长
+ failover_auth_epoch：当前正在执行的故障转移的任期
+ currentEpoch：当前的集群任期
+ lastVoteEpoch：当前节点最新同意投票的任期

在 cluster 集群模式下，当一个主节点下线之后，只能由该节点的从节点发起选举流程，并执行故障转移操作。故障转移完成后，该故障转移的从节点会成为新的主节点。从节点会在 `clusterCron` 中调用 `clusterHandleSlaveFailover` 判断是否需要发起故障转移

```C
//src/cluster.c
void clusterHandleSlaveFailover(void) {
    /*...*/
    auth_timeout = server.cluster_node_timeout*2;
    if (auth_timeout < 2000) auth_timeout = 2000;
    auth_retry_time = auth_timeout*2;

    /* Pre conditions to run the function, that must be met both in case
     * of an automatic or manual failover:
     * 1) We are a slave.
     * 2) Our master is flagged as FAIL, or this is a manual failover.
     * 3) We don't have the no failover configuration set, and this is
     *    not a manual failover.
     * 4) It is serving slots. */
    if (nodeIsMaster(myself) ||//当前节点是主节点
        myself->slaveof == NULL ||//当前节点是主节点
        (!nodeFailed(myself->slaveof) && !manual_failover) ||//当前节点的主节点不是客观下线状态并且没有执行手动故障恢复
        (server.cluster_slave_no_failover && !manual_failover) ||//当前节点开启了禁止故障转移的选项并且没有执行手动故障恢复
        myself->slaveof->numslots == 0)//当前节点的主节点不负责任何槽位
    {//满足以上任意条件，则不执行故障转移
        /* There are no reasons to failover, so we set the reason why we
         * are returning without failing over to NONE. */
        server.cluster->cant_failover_reason = CLUSTER_CANT_FAILOVER_NONE;
        return;
    }
    /*...*/

    /* If the previous failover attempt timeout and the retry time has
     * elapsed, we can setup a new one. */
    if (auth_age > auth_retry_time) {//auth_age = mstime() - server.cluster->failover_auth_time，表示本次选举已花费的时间，如果大于 auth_retry_time 则说明本次选举失败，重新准备下次选举
        server.cluster->failover_auth_time = mstime() +
            500 + /* Fixed delay of 500 milliseconds, let FAIL msg propagate. */
            random() % 500; /* Random delay between 0 and 500 milliseconds. */
        server.cluster->failover_auth_count = 0;//重置已收到的投票数
        server.cluster->failover_auth_sent = 0;//重置请求发送标志
        server.cluster->failover_auth_rank = clusterGetSlaveRank();//重新计算优先级
        /* We add another delay that is proportional to the slave rank.
         * Specifically 1 second * rank. This way slaves that have a probably
         * less updated replication offset, are penalized. */
        server.cluster->failover_auth_time +=
            server.cluster->failover_auth_rank * 1000;//给 failover_auth_time 增加一个与节点优先级有关的选举开始延迟，优先级越低的节点发起选举的时间越晚
        /* However if this is a manual failover, no delay is needed. */
        if (server.cluster->mf_end) {
            server.cluster->failover_auth_time = mstime();
            server.cluster->failover_auth_rank = 0;
	    clusterDoBeforeSleep(CLUSTER_TODO_HANDLE_FAILOVER);
        }
        serverLog(LL_NOTICE,
            "Start of election delayed for %lld milliseconds "
            "(rank #%d, offset %lld).",
            server.cluster->failover_auth_time - mstime(),
            server.cluster->failover_auth_rank,
            replicationGetSlaveOffset());
        /* Now that we have a scheduled election, broadcast our offset
         * to all the other slaves so that they'll updated their offsets
         * if our offset is better. */
        clusterBroadcastPong(CLUSTER_BROADCAST_LOCAL_SLAVES);
        return;
    }

    /*...*/
    /* Return ASAP if we can't still start the election. */
    if (mstime() < server.cluster->failover_auth_time) {//当前时间小于选举开始时间表明本次选举失败，下次选举未开始，退出
        clusterLogCantFailover(CLUSTER_CANT_FAILOVER_WAITING_DELAY);
        return;
    }

    /* Return ASAP if the election is too old to be valid. */
    if (auth_age > auth_timeout) {//选举已花费时间大于选举超时时间说明本次选举失败，退出
        clusterLogCantFailover(CLUSTER_CANT_FAILOVER_EXPIRED);
        return;
    }
    
    /* Ask for votes if needed. */
    if (server.cluster->failover_auth_sent == 0) {//条件成立说明当前节点还没有发送投票请求
        server.cluster->currentEpoch++;//当前任期+1，进入下一任期
        server.cluster->failover_auth_epoch = server.cluster->currentEpoch;//更新 failover_auth_epoch
        serverLog(LL_NOTICE,"Starting a failover election for epoch %llu.",
            (unsigned long long) server.cluster->currentEpoch);
        clusterRequestFailoverAuth();//在集群中广播发送 CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST 消息，要求其他节点给自己投票
        server.cluster->failover_auth_sent = 1;//投票标识置为1
        clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG|
                             CLUSTER_TODO_UPDATE_STATE|
                             CLUSTER_TODO_FSYNC_CONFIG);
        return; /* Wait for replies. */
    }

    /* Check if we reached the quorum. */
    if (server.cluster->failover_auth_count >= needed_quorum) {//检查当前节点获得的票数是否达到要求
        /* We have the quorum, we can finally failover the master. */

        serverLog(LL_NOTICE,
            "Failover election won: I'm the new master.");

        /* Update my configEpoch to the epoch of the election. */
        if (myself->configEpoch < server.cluster->failover_auth_epoch) {
            myself->configEpoch = server.cluster->failover_auth_epoch;
            serverLog(LL_NOTICE,
                "configEpoch set to %llu after successful failover",
                (unsigned long long) myself->configEpoch);
        }

        /* Take responsibility for the cluster slots. */
        clusterFailoverReplaceYourMaster();//开始故障转移
    } else {
        clusterLogCantFailover(CLUSTER_CANT_FAILOVER_WAITING_VOTES);
    }
}
```

当前节点为从节点，所以不会给自己投票。

其他节点在收到 `CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST` 消息后会进行处理（处理函数为 `clusterProcessPacket`）

```C
//src/cluster.c +2816
    if (sender && !nodeInHandshake(sender)) {
        /* Update our currentEpoch if we see a newer epoch in the cluster. */
        senderCurrentEpoch = ntohu64(hdr->currentEpoch);
        senderConfigEpoch = ntohu64(hdr->configEpoch);
        if (senderCurrentEpoch > server.cluster->currentEpoch)//如果发送节点的任期比接收节点的任期更大，则更新任期号，包括接收节点的 currentEpoch 属性以及接收节点视图下发送节点实例的 configEpoch
            server.cluster->currentEpoch = senderCurrentEpoch;
        /* Update the sender configEpoch if it is publishing a newer one. */
        if (senderConfigEpoch > sender->configEpoch) {
            sender->configEpoch = senderConfigEpoch;
            clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG|
                                 CLUSTER_TODO_FSYNC_CONFIG);
        }
        /* Update the replication offset info for this node. */
        sender->repl_offset = ntohu64(hdr->offset);//更新接收节点视图下发送节点实例的复制偏移量（主节点发送 server.master_repl_offset，从节点发送 server.master.reploff），用于故障转移时计算节点优先级。
        sender->repl_offset_time = now;
        /*...*/
    }

//src/cluster.c +3163
    } else if (type == CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST) {
        if (!sender) return 1;  /* We don't know that node. */
        clusterSendFailoverAuthIfNeeded(sender,hdr);
```

调用了 `clusterSendFailoverAuthIfNeeded`

```C
//src/cluster.c
/*
	以下从接收节点的角度分析
*/
void clusterSendFailoverAuthIfNeeded(clusterNode *node, clusterMsg *request) {
    clusterNode *master = node->slaveof;
    uint64_t requestCurrentEpoch = ntohu64(request->currentEpoch);
    uint64_t requestConfigEpoch = ntohu64(request->configEpoch);
    unsigned char *claimed_slots = request->myslots;
    int force_ack = request->mflags[0] & CLUSTERMSG_FLAG0_FORCEACK;
    int j;

    /* IF we are not a master serving at least 1 slot, we don't have the
     * right to vote, as the cluster size in Redis Cluster is the number
     * of masters serving at least one slot, and quorum is the cluster
     * size + 1 */
    if (nodeIsSlave(myself) || myself->numslots == 0) return;//从节点或者不负责槽位的主节点没有投票资格

    /* Request epoch must be >= our currentEpoch.
     * Note that it is impossible for it to actually be greater since
     * our currentEpoch was updated as a side effect of receiving this
     * request, if the request epoch was greater. */
    if (requestCurrentEpoch < server.cluster->currentEpoch) {//请求节点的任期号必须不小于接收节点的任期号，否则拒绝给请求节点投票
        serverLog(LL_WARNING,
            "Failover auth denied to %.40s (%s): reqEpoch (%llu) < curEpoch(%llu)",
            node->name, node->human_nodename,
            (unsigned long long) requestCurrentEpoch,
            (unsigned long long) server.cluster->currentEpoch);
        return;
    }

    /* I already voted for this epoch? Return ASAP. */
    if (server.cluster->lastVoteEpoch == server.cluster->currentEpoch) {//已经给其他节点投过票则退出
        serverLog(LL_WARNING,
                "Failover auth denied to %.40s (%s): already voted for epoch %llu",
                node->name, node->human_nodename,
                (unsigned long long) server.cluster->currentEpoch);
        return;
    }

    /* Node must be a slave and its master down.
     * The master can be non failing if the request is flagged
     * with CLUSTERMSG_FLAG0_FORCEACK (manual failover). */
    if (nodeIsMaster(node) || master == NULL ||//发送节点是主节点或者不确定其主节点或者其主节点未下线则拒绝给其投票
        (!nodeFailed(master) && !force_ack))
    {
        if (nodeIsMaster(node)) {
            serverLog(LL_WARNING,
                    "Failover auth denied to %.40s (%s): it is a master node",
                    node->name, node->human_nodename);
        } else if (master == NULL) {
            serverLog(LL_WARNING,
                    "Failover auth denied to %.40s (%s): I don't know its master",
                    node->name, node->human_nodename);
        } else if (!nodeFailed(master)) {
            serverLog(LL_WARNING,
                    "Failover auth denied to %.40s (%s): its master is up",
                    node->name, node->human_nodename);
        }
        return;
    }

    /* We did not voted for a slave about this master for two
     * times the node timeout. This is not strictly needed for correctness
     * of the algorithm but makes the base case more linear. */
    if (mstime() - node->slaveof->voted_time < server.cluster_node_timeout * 2)//接收节点上次给该从节点所属的主节点下的从节点投票后经过的时间小于 cluster_node_timeout 的2倍则拒绝投票
    {
        serverLog(LL_WARNING,
                "Failover auth denied to %.40s %s: "
                "can't vote about this master before %lld milliseconds",
                node->name, node->human_nodename,
                (long long) ((server.cluster_node_timeout*2)-
                             (mstime() - node->slaveof->voted_time)));
        return;
    }

    /* The slave requesting the vote must have a configEpoch for the claimed
     * slots that is >= the one of the masters currently serving the same
     * slots in the current configuration. */
    for (j = 0; j < CLUSTER_SLOTS; j++) {//这里，检查发起投票的从节点所声明的槽位的原负责节点的任期号和是否小于等于从节点的任期号，如果有大于的情况则拒绝投票
        if (bitmapTestBit(claimed_slots, j) == 0) continue;
        if (isSlotUnclaimed(j) ||
            server.cluster->slots[j]->configEpoch <= requestConfigEpoch)
        {
            continue;
        }
        /* If we reached this point we found a slot that in our current slots
         * is served by a master with a greater configEpoch than the one claimed
         * by the slave requesting our vote. Refuse to vote for this slave. */
        serverLog(LL_WARNING,
                "Failover auth denied to %.40s (%s): "
                "slot %d epoch (%llu) > reqEpoch (%llu)",
                node->name, node->human_nodename, j,
                (unsigned long long) server.cluster->slots[j]->configEpoch,
                (unsigned long long) requestConfigEpoch);
        return;
    }

    /* We can vote for this slave. *///执行到这里，说明可以给发送节点投票
    server.cluster->lastVoteEpoch = server.cluster->currentEpoch;
    node->slaveof->voted_time = mstime();
    clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG|CLUSTER_TODO_FSYNC_CONFIG);
    clusterSendFailoverAuth(node);//发送投票确认消息 CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK，发送节点收到该消息后会将 server.cluster->failover_auth_count 加1
    serverLog(LL_NOTICE, "Failover auth granted to %.40s (%s) for epoch %llu",
        node->name, node->human_nodename, (unsigned long long) server.cluster->currentEpoch);
}
```

### 从节点晋升

经过选举，下线主节点的某个从节点会成为 leader，该节点完成故障转移后会成为新的 master 节点。完成这一工作的函数为 `clusterFailoverReplaceYourMaster`

```C
//src/cluster.c
void clusterFailoverReplaceYourMaster(void) {
    int j;
    clusterNode *oldmaster = myself->slaveof;

    if (nodeIsMaster(myself) || oldmaster == NULL) return;

    /* 1) Turn this node into a master. */
    clusterSetNodeAsMaster(myself);//在集群视图中，将当前节点设置为主节点
    replicationUnsetMaster();//从主从复制的角度取消当前节点的主从复制关系并将自身设置为主节点

    /* 2) Claim all the slots assigned to our master. */
    for (j = 0; j < CLUSTER_SLOTS; j++) {//将原主节点负责的槽位指派给当前节点
        if (clusterNodeGetSlotBit(oldmaster,j)) {
            clusterDelSlot(j);
            clusterAddSlot(myself,j);
        }
    }

    /* 3) Update state and save config. */
    clusterUpdateState();//更新集群状态并写入数据文件
    clusterSaveConfigOrDie(1);

    /* 4) Pong all the other nodes so that they can update the state
     *    accordingly and detect that we switched to master role. */
    clusterBroadcastPong(CLUSTER_BROADCAST_ALL);//发送PONG消息将当前节点的最新信息发送给其他节点，其他节点收到消息后更新自身的集群视图或者与晋升节点建立主从关系

    /* 5) If there was a manual failover in progress, clear the state. */
    resetManualFailover();
}
```

### 更新集群信息

最后看一下集群中其他节点在完成故障转移后的处理逻辑（在函数 clusterProcessPacket 中）。

现在假设集群中有A、B、C三个节点，A为主节点，B、C为从节点，A节点下线，B节点晋升为主节点

```C
//src/cluster.c +2902
    if (type == CLUSTERMSG_TYPE_PING || type == CLUSTERMSG_TYPE_PONG ||
        type == CLUSTERMSG_TYPE_MEET)
    {
        /*....*/
        if (sender) {
            if (!memcmp(hdr->slaveof,CLUSTER_NODE_NULL_NAME,
                sizeof(hdr->slaveof)))//消息中的 slaveof 为 CLUSTER_NODE_NULL_NAME 说明发送节点为主节点，则将发送节点在本节点中的实例角色切换为主节点。根据上面的例子，其他节点在收到B发送的PONG消息后，会将B切换为主节点
            {
                /* Node is a master. */
                clusterSetNodeAsMaster(sender);
            } else {
                /* Node is a slave. *///原本为主节点的节点声明自己为从节点。根据上面的例子，如果A节点重新上线，此时B节点已经晋升为主节点，A节点会成为B节点的从节点，其他节点在收到A的PONG消息后会将A的状态修改为从节点，并且设定B为A的主节点
                clusterNode *master = clusterLookupNode(hdr->slaveof, CLUSTER_NAMELEN);

                if (nodeIsMaster(sender)) {
                    /* Master turned into a slave! Reconfigure the node. */
                    clusterDelNodeSlots(sender);//清除该节点负责的槽位
                    sender->flags &= ~(CLUSTER_NODE_MASTER| //更新节点标识
                                       CLUSTER_NODE_MIGRATE_TO);
                    sender->flags |= CLUSTER_NODE_SLAVE;

                    /* Update config and state. */
                    clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG|
                                         CLUSTER_TODO_UPDATE_STATE);
                }

                /* Master node changed for this slave? */
                if (master && sender->slaveof != master) {//发送节点为从节点，但是其声明的主节点与记录的不一致，修改其主节点。其他节点在收到C发送的PONG之后会将C的主节点从A修改为B
                    if (sender->slaveof)
                        clusterNodeRemoveSlave(sender->slaveof,sender);
                    clusterNodeAddSlave(master,sender);
                    sender->slaveof = master;

                    /* Update config. */
                    clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG);
                }
            }
        }

        /* Update our info about served slots.
         *
         * Note: this MUST happen after we update the master/slave state
         * so that CLUSTER_NODE_MASTER flag will be set. */

        /* Many checks are only needed if the set of served slots this
         * instance claims is different compared to the set of slots we have
         * for it. Check this ASAP to avoid other computational expansive
         * checks later. */
        clusterNode *sender_master = NULL; /* Sender or its master if slave. */
        int dirty_slots = 0; /* Sender claimed slots don't match my view? */

        if (sender) {//集群节点发送消息时会将主节点的槽位位图添加到请求中。这里接收节点将收到的位图与自身的节点实例的位图作对比，检查是否一致。在上面的例子中B负责的槽位发生了变化（原来是从节点不负责槽位，现在声明自己负责部分槽位）
            sender_master = nodeIsMaster(sender) ? sender : sender->slaveof;
            if (sender_master) {
                dirty_slots = memcmp(sender_master->slots,
                        hdr->myslots,sizeof(hdr->myslots)) != 0;
            }
        }

        /* 1) If the sender of the message is a master, and we detected that
         *    the set of slots it claims changed, scan the slots to see if we
         *    need to update our configuration. */
        if (sender && nodeIsMaster(sender) && dirty_slots)//如果发送节点是主节点，并且槽位位图信息发生了变化，则更新槽位信息或者建立主从关系
            clusterUpdateSlotsConfigWith(sender,senderConfigEpoch,hdr->myslots);
```

### 建立主从关系

`clusterUpdateSlotsConfigWith` 负责更新槽位信息或建立主从关系

```C
//src/cluster.c
void clusterUpdateSlotsConfigWith(clusterNode *sender, uint64_t senderConfigEpoch, unsigned char *slots) {
    /*...*/
    
    for (j = 0; j < CLUSTER_SLOTS; j++) {
        if (bitmapTestBit(slots,j)) {
            sender_slots++;

            /* The slot is already bound to the sender of this message. */
            if (server.cluster->slots[j] == sender) {//如果槽位j已经被指派给发送节点，清除无主槽位位图
                bitmapClearBit(server.cluster->owner_not_claiming_slot, j);
                continue;
            }

            /* The slot is in importing state, it should be modified only
             * manually via redis-cli (example: a resharding is in progress
             * and the migrating side slot was already closed and is advertising
             * a new config. We still want the slot to be closed manually). */
            if (server.cluster->importing_slots_from[j]) continue;

            /* We rebind the slot to the new node claiming it if:
             * 1) The slot was unassigned or the previous owner no longer owns the slot or
             *    the new node claims it with a greater configEpoch.
             * 2) We are not currently importing the slot. */
            if (isSlotUnclaimed(j) ||
                server.cluster->slots[j]->configEpoch < senderConfigEpoch)//如果在当前节点的视图下，槽位处于无主状态或者槽位负责节点的任期小于发送节点，则执行以下操作
            {
                /* Was this slot mine, and still contains keys? Mark it as
                 * a dirty slot. */
                if (server.cluster->slots[j] == myself &&
                    countKeysInSlot(j) &&
                    sender != myself)
                {
                    dirty_slots[dirty_slots_count] = j;
                    dirty_slots_count++;
                }

                if (server.cluster->slots[j] == curmaster) {//如果在当前节点视图下槽位的原指派节点是当前节点的主节点（如果当前节点是主节点就是当前节点本身），然而在发送节点发送的信息中表明其负责该节点，则将发送节点赋值给 newmaster，说明当前节点的主节点可能发生变更（或者当前节点本来是主节点但是发生故障转移）
                    newmaster = sender;
                    migrated_our_slots++;//统计当前节点（当前节点是主节点）或者当前节点的原主节点（当前节点是从节点）不再负责的槽位数量
                }
                clusterDelSlot(j);//更新槽位指派数组和槽位位图。在上面的例子中，C会删除A负责的槽位并指派给B
                clusterAddSlot(sender,j);
                bitmapClearBit(server.cluster->owner_not_claiming_slot, j);
                clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG|
                                     CLUSTER_TODO_UPDATE_STATE|
                                     CLUSTER_TODO_FSYNC_CONFIG);
            }
        } else if /*...*/
    }

    /* After updating the slots configuration, don't do any actual change
     * in the state of the server if a module disabled Redis Cluster
     * keys redirections. */
    if (server.cluster_module_flags & CLUSTER_MODULE_FLAG_NO_REDIRECTION)
        return;

    /* If at least one slot was reassigned from a node to another node
     * with a greater configEpoch, it is possible that:
     * 1) We are a master left without slots. This means that we were
     *    failed over and we should turn into a replica of the new
     *    master.
     * 2) We are a slave and our master is left without slots. We need
     *    to replicate to the new slots owner. *///1）当前节点是一个没有槽位的主节点。这意味着发生了故障转移，当前节点应该变成新主节点的副本；2）当前节点是一个从节点但是主节点没有负责的槽位了，需要成为新的主节点的从节点。此时当前节点与发送节点建立主从关系。在上面的例子中，C会与B建立主从关系
    if (newmaster && curmaster->numslots == 0 && 
            (server.cluster_allow_replica_migration ||
             sender_slots == migrated_our_slots)) {
        serverLog(LL_NOTICE,
            "Configuration change detected. Reconfiguring myself "
            "as a replica of %.40s (%s)", sender->name, sender->human_nodename);
        clusterSetMaster(sender);
        clusterDoBeforeSleep(CLUSTER_TODO_SAVE_CONFIG|
                             CLUSTER_TODO_UPDATE_STATE|
                             CLUSTER_TODO_FSYNC_CONFIG);
    } else if /*...*/
}
```

### 从节点迁移

cluster 集群模式下，集群中不应该出现孤儿主节点（没有有效从节点的主节点），否则这样的主节点故障后无法进行故障转移。为此，`clusterCron` 函数中还由从节点迁移的逻辑。该逻辑在从节点上执行。如果某个从节点满足如下条件则执行从节点迁移

1. 当前集群中存在孤儿节点
2. 当前从节点所属的主节点是从节点最多的主节点之一，并且大于 `cluster_migration_barrier` 配置
3. 当前从节点的 name 比其主节点下所有其他从节点的 name 都小
4. `cluster_allow_replica_migration` 配置为 yes

满足上述条件，当前节点会迁移成为孤儿主节点的从节点，这样可以尽量避免集群中出现孤儿节点，相关逻辑在 `clusterHandleSlaveMigration` 中实现
