# 后台删除

redis 的主要逻辑是单线程执行的。如果在执行过程中删除键的时候导致主线程阻塞过长的时间，redis 提供了 unlink 命令用于键的非阻塞删除。

## 命令实现

从 redis4 开始提供 unlink 命令实现非阻塞删除，该命令的实现函数为 `delCommand`，最终调用了 `dbGenericDelete` 实现非阻塞删除

```C
//src/db.c
int dbGenericDelete(redisDb *db, robj *key, int async, int flags) {
    dictEntry **plink;
    int table;
    dictEntry *de = dictTwoPhaseUnlinkFind(db->dict,key->ptr,&plink,&table);
    if (de) {
        /*...*/
        if (async) {//参数 async 表明要非阻塞删除
            /* Because of dbUnshareStringValue, the val in de may change. */
            freeObjAsync(key, dictGetVal(de), db->id);
            dictSetVal(db->dict, de, NULL);//将值对象置为 NULL，确保主线程中无法在访问该值
        }
        /*...*/
        dictTwoPhaseUnlinkFree(db->dict,de,plink,table);//释放 key 以及 value，如果这里是非阻塞删除，这里的 value 已经是 nULL，不会阻塞
        return 1;
    } else {
        return 0;
    }
}

src/lazyfree.c
#define LAZYFREE_THRESHOLD 64

/* Free an object, if the object is huge enough, free it in async way. */
void freeObjAsync(robj *key, robj *obj, int dbid) {
    size_t free_effort = lazyfreeGetFreeEffort(key,obj,dbid);
    /* Note that if the object is shared, to reclaim it now it is not
     * possible. This rarely happens, however sometimes the implementation
     * of parts of the Redis core may call incrRefCount() to protect
     * objects, and then call dbDelete(). */
    if (free_effort > LAZYFREE_THRESHOLD && obj->refcount == 1) {//对象中的元素数量超过 LAZYFREE_THRESHOLD 并且引用数量为1，则执行非阻塞删除
        atomicIncr(lazyfree_objects,1);
        bioCreateLazyFreeJob(lazyfreeFreeObject,1,obj);//创建一个后台线程负责对象删除
    } else {
        decrRefCount(obj);
    }
}
```

## 后台线程

redis 中的后台线程负责完成耗时操作。

### 初始化

```C
//src/bio.c
static char* bio_worker_title[] = {
    "bio_close_file",
    "bio_aof",
    "bio_lazy_free",
};

#define BIO_WORKER_NUM (sizeof(bio_worker_title) / sizeof(*bio_worker_title)) //这里 BIO_WORKER_NUM 为3

void bioInit(void) {
    pthread_attr_t attr;
    pthread_t thread;
    size_t stacksize;
    unsigned long j;

    /* Initialization of state vars and objects */
    for (j = 0; j < BIO_WORKER_NUM; j++) {//初始化三类后台线程的相关的互斥锁和条件变量、任务队列
        pthread_mutex_init(&bio_mutex[j],NULL);
        pthread_cond_init(&bio_newjob_cond[j],NULL);
        bio_jobs[j] = listCreate();
    }

    /* Set the stack size as by default it may be small in some system *///设置栈大小避免在某些系统上出错
    /*..*/

    /* Ready to spawn our threads. We use the single argument the thread
     * function accepts in order to pass the job ID the thread is
     * responsible for. */
    for (j = 0; j < BIO_WORKER_NUM; j++) {
        void *arg = (void*)(unsigned long) j;
        if (pthread_create(&thread,&attr,bioProcessBackgroundJobs,arg) != 0) {//创建后台线程，线程启动后执行 bioProcessBackgroundJobs，参数为 j
            serverLog(LL_WARNING, "Fatal: Can't initialize Background Jobs. Error message: %s", strerror(errno));
            exit(1);
        }
        bio_threads[j] = thread;
    }
}
```

### 添加任务

针对三类后台任务，redis 提供了 `bioCreateXXXJob` 的函数用于添加相应的后台任务，以非阻塞删除为例

```C
//src/bio.c
/*
	free_fn：回调函数，后台线程中调用
*/
void bioCreateLazyFreeJob(lazy_free_fn free_fn, int arg_count, ...) {
    va_list valist;
    /* Allocate memory for the job structure and all required
     * arguments */
    bio_job *job = zmalloc(sizeof(*job) + sizeof(void *) * (arg_count));
    job->free_args.free_fn = free_fn;

    va_start(valist, arg_count);
    for (int i = 0; i < arg_count; i++) {
        job->free_args.free_args[i] = va_arg(valist, void *);
    }
    va_end(valist);
    bioSubmitJob(BIO_LAZY_FREE, job);
}

void bioSubmitJob(int type, bio_job *job) {
    job->header.type = type;
    unsigned long worker = bio_job_to_worker[type];
    pthread_mutex_lock(&bio_mutex[worker]);
    listAddNodeTail(bio_jobs[worker],job);//向对应的任务队列中追加任务
    bio_jobs_counter[type]++;
    pthread_cond_signal(&bio_newjob_cond[worker]);//唤醒对应的后台线程，开始执行任务
    pthread_mutex_unlock(&bio_mutex[worker]);//释放互斥锁
}
```

### 任务执行

后台线程的主逻辑在 `bioProcessBackgroundJobs` 中

```C
//src/bio.c
void *bioProcessBackgroundJobs(void *arg) {
    /*...*/

    pthread_mutex_lock(&bio_mutex[worker]);//获取互斥锁
    /* Block SIGALRM so we are sure that only the main thread will
     * receive the watchdog signal. */
    sigemptyset(&sigset);
    sigaddset(&sigset, SIGALRM);
    if (pthread_sigmask(SIG_BLOCK, &sigset, NULL))
        serverLog(LL_WARNING,
            "Warning: can't mask SIGALRM in bio.c thread: %s", strerror(errno));

    while(1) {
        listNode *ln;

        /* The loop always starts with the lock hold. */
        if (listLength(bio_jobs[worker]) == 0) {
            pthread_cond_wait(&bio_newjob_cond[worker], &bio_mutex[worker]);//进入等待状态，暂时释放互斥锁，返回后自动重新获得互斥锁
            continue;
        }
        /* Get the job from the queue. */
        ln = listFirst(bio_jobs[worker]);//从任务队列中获取任务
        job = ln->value;
        /* It is now possible to unlock the background system as we know have
         * a stand alone job structure to process.*/
        pthread_mutex_unlock(&bio_mutex[worker]);//释放互斥锁，允许主线程继续添加任务

        /* Process the job accordingly to its type. */
        int job_type = job->header.type;

        if (job_type == BIO_CLOSE_FILE) {
            /*根据具体的任务类型执行不同的处理逻辑*/
        zfree(job);

        /* Lock again before reiterating the loop, if there are no longer
         * jobs to process we'll block again in pthread_cond_wait(). */
        pthread_mutex_lock(&bio_mutex[worker]);//重新获取互斥锁，删除已完成的任务并重新开始循环
        listDelNode(bio_jobs[worker], ln);
        bio_jobs_counter[job_type]--;
        pthread_cond_signal(&bio_newjob_cond[worker]);
    }
}
```

